{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import theano\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "from skorch.net import NeuralNetClassifier\n",
    "import torch.utils.data as Data\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # If CUDA is available => cuda:0 prints\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid the feature matrix\n",
    "data = pd.read_csv('/home/xsong/Alma/2017---Deep-learning-yeast-UTRs/Data/Random_UTRs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of the sequences.\n",
    "\n",
    "i.e. we're converting the sequences from being represented as a 50 character string of bases to a 4x50 matrix of 1's and 0's, with each row corresponding to a base and every column a position in the UTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the work of Cuperus et al.\n",
    "# one hot encoding of UTRs\n",
    "# X = one hot encoding matrix\n",
    "# Y = growth rates\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "\n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "\n",
    "    n = len(df)\n",
    "    \n",
    "    # length of the UTR sequence\n",
    "    # we also add 10 empty spaces to either side\n",
    "    total_width = df[seq_column].str.len().max() + 20\n",
    "    \n",
    "    # initialize an empty numpy ndarray of the appropriate size\n",
    "    X = np.zeros((n, 1, 4, total_width))\n",
    "    \n",
    "    # an array with the sequences that we will one-hot encode\n",
    "    seqs = df[seq_column].values\n",
    "    \n",
    "    # loop through the array of sequences to create an array that keras will actually read\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        \n",
    "        # loop through each individual sequence, from the 5' to 3' end\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[i, 0, base_dict[seq[b]], int(b + round((total_width - len(seq))/2.))] = 1.\n",
    "    \n",
    "        # keep track of where we are\n",
    "        if (i%100000)==0:\n",
    "            print(i),\n",
    "        \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    Y = np.array(df[expression].values,\n",
    "                   dtype = theano.config.floatX)[:, np.newaxis]\n",
    "    \n",
    "    return X, Y, total_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y, total_width = one_hot_encoding(data, 'UTR', 'growth_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.from_numpy(X).float().cuda() #change to torch and upload to CUDA\n",
    "Y_torch = torch.from_numpy(Y).float().cuda() #change to torch and upload to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sorted numpy array of UTR indexes, from least reads to most reads\n",
    "sorted_inds = data.sort_values('t0').index.values\n",
    "train_inds = sorted_inds[:int(0.95*len(sorted_inds))] # 95% of the data as the training set\n",
    "test_inds = sorted_inds[int(0.95*len(sorted_inds)):] # UTRs with most reads at time point 0 as the test set\n",
    "\n",
    "# set the seed before randomly shuffling the data\n",
    "seed = 0.5\n",
    "random.shuffle(train_inds, lambda :seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model\n",
    "\n",
    "I need to figure out how to make the dropout happen and Flatten. \n",
    "How do hidden units work in fully connected layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buid the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=1\n",
    "batch_size=10\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super(Net, self).__init__()\n",
    "        # input channel, output channels = number of filters, convolution kernel size\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, size, [4,13])\n",
    "        self.conv2 = nn.Conv2d(1, size, [1,13])\n",
    "        self.conv3 = nn.Conv2d(1, size, [1,13])\n",
    "        self.fc1 = nn.Linear(34, 120)\n",
    "        self.lin_out1 = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print('conv1',x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print('conv2',x.size())\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print('conv3',x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print('fc1',x.size())\n",
    "        x = self.lin_out1(x)\n",
    "        #print('lol1',x.size())\n",
    "        #x = nn.Dropout(p=0.15)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net(data)\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "next(net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, data):\n",
    "        net.eval()\n",
    "        ######EVALUATION STEP#################\n",
    "        #same as during training step, need to do batch training\n",
    "        correct_count = 0.\n",
    "        total_count = 0.\n",
    "        for i, (dev_data_x,dev_data_y) in enumerate(dev_loader):\n",
    "            dev_data_y.requires_grad=False\n",
    "            dev_data_y = dev_data_y[:,0]   # resize the value as vector\n",
    "            pred = net(dev_data_x)\n",
    "            pred = pred[:,0,0,0]   #resize the output as vector\n",
    "            fit= loss_func(pred, dev_data_y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choice of optimizer & loss function => MSE \n",
    "# Using backpropagation\n",
    "\n",
    "# Initiate the hyperparameters\n",
    "number_epochs = 20\n",
    "track_loss = []\n",
    "loss_func = nn.MSELoss().cuda()\n",
    "k_fold = 10\n",
    "\n",
    "# Define dataset and initialize mini-batch data\n",
    "x = X_torch[train_inds]\n",
    "y = Y_torch[train_inds]\n",
    "num_data = x.shape[0]\n",
    "num_dev_data = int(num_data/k_fold) #evalutation data amount\n",
    "fit_store = np.tile(0,(k_fold,number_epochs))\n",
    "fit_store = torch.from_numpy(fit_store).float().cuda()\n",
    "batch_size=32\n",
    "\n",
    "print(num_data - num_dev_data, num_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training with crossvalidation\n",
    "best_save=np.zeros((k_fold), float)\n",
    "for split in range(k_fold):\n",
    "    train_index = np.empty\n",
    "    dev_index = np.empty\n",
    "    dev_index = list(range(num_dev_data*split, num_dev_data*(split+1)))\n",
    "    if split == 0:\n",
    "        train_index = np.array(range(num_dev_data*(split+1),num_dev_data*k_fold))\n",
    "    else:\n",
    "        train_index = np.array(range(0, num_dev_data*split))\n",
    "        train_index = np.append(train_index, np.array(range(num_dev_data*(split+1),num_dev_data*k_fold)))\n",
    "        \n",
    "    dev_data_x = x[dev_index]      #evalutation data\n",
    "    dev_data_y = y[dev_index]      #evalutation data\n",
    "\n",
    "    train_data_x = x[train_index]\n",
    "    train_data_y = y[train_index]\n",
    "    \n",
    "    train_dataset = Data.TensorDataset(train_data_x, train_data_y)\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    dev_dataset = Data.TensorDataset(dev_data_x, dev_data_y)\n",
    "    dev_loader = Data.DataLoader(dev_dataset, batch_size, shuffle=True)  \n",
    "    \n",
    "    learning_rate = 0.005\n",
    "    running_loss = 0.\n",
    "    best_fit = 10\n",
    "    optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "    for epoch in range(number_epochs): # loop over the dataset multiple time        \n",
    "        #######TRAINING STEP##################\n",
    "        j=1\n",
    "        for i, (train_data_x,train_data_y) in enumerate(train_loader):\n",
    "            train_data_y.requires_grad=False\n",
    "            train_data_y = train_data_y[:,0].float()                 # resize the value as vector\n",
    "            optimizer.zero_grad()      # zero the parameter gradients\n",
    "            output = net(train_data_x)\n",
    "            output = output[:,0,0,0]   #resize the output as vector\n",
    "            loss = loss_func(output, train_data_y)# compute the loss of the system\n",
    "            loss.backward()            # start backward function\n",
    "            optimizer.step()           # optimizing step\n",
    "            running_loss += loss.item()\n",
    "            if ((i+1)%1000)==0:\n",
    "                print(epoch+1, i+1 ,running_loss / 1000)\n",
    "                track_loss.append(running_loss / 1000)\n",
    "                running_loss=0.\n",
    "        fit = eval(net, dev_data_x)\n",
    "        fit_store[split,epoch]=fit\n",
    "        print(epoch+1,fit[0])\n",
    "        if fit<best_fit:\n",
    "            best_fit = fit\n",
    "            print(\"SAVE FILE AS Model_training_28-06-18_\" + str(split+1) + \"_\" + str(epoch+1))\n",
    "            torch.save(net.state_dict(),\"/home/xsong/Alma/Training/Model_training_01-07-18_\" + str(split+1) + \"_\" + str(epoch+1) + \".pt\")\n",
    "            best_save(split) = epoch\n",
    "        else:\n",
    "            learning_rate *= 0.8\n",
    "            optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting of the loss function\n",
    "time = np.array(range(0, number_epochs))\n",
    "time = np.tile(time, (10,1))\n",
    "fit_plot = fit_store.data.cpu().numpy()\n",
    "print(fit_plot)\n",
    "plt.plot(fit_plot)\n",
    "plt.axis([1,20, 0.5, 2])\n",
    "plt.ylabel('Value of the loss function')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_plot_t)\n",
    "plt.ylabel('Value of the loss function')\n",
    "plt.xlabel('Epochs')\n",
    "#plt.show()\n",
    "plt.savefig(\"/home/xsong/Desktop/test_loss.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## reorganize running_loss function\n",
    "loss_plot = np.reshape(track_loss, (200,3))\n",
    "loss_mean = loss_plot.mean(axis=1)\n",
    "print(loss_mean)\n",
    "loss_mean = np.reshape(loss_mean,(10,20))\n",
    "loss_mean = np.transpose(loss_mean)\n",
    "print(loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of the loss function\n",
    "time = np.array(range(0, number_epochs))\n",
    "time = np.tile(time, (10,1))\n",
    "#running_loss_plot = running_loss.data.cpu().numpy()\n",
    "#running_loss_plot = np.ndarray.transpose(running_loss_plot())\n",
    "plt.plot(loss_mean)\n",
    "plt.ylabel('Value of the loss function')\n",
    "plt.xlabel('Epochs')\n",
    "plt.axis([1,20, 1.195, 1.265])\n",
    "#plt.show()\n",
    "plt.savefig(\"/home/xsong/Desktop/running.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "\n",
    "During testing time, I load all ten models and keep them in a list. When making a prediction on a single instance, I apply each of the 10 models to give scores, and add the predicted scores up to make a single prediction. I do argmax over the summed scores to make the final prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = X_torch[test_inds]\n",
    "val_y = Y_torch[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "best_save = [9,32,49,7,36,24,36,4,49,4]\n",
    "for split in range(k_fold):\n",
    "    model = Net(data)\n",
    "    model.load_state_dict(torch.load(\"/home/xsong/Alma/Training/50_epochs_training/Model_training_01-07_\" + str(split+1) + \"_\" + str(best_save[split]) + \".pt\"))\n",
    "    models.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Data.TensorDataset(val_x, val_y)\n",
    "test_loader = Data.DataLoader(test_dataset, batch_size, shuffle=True)  \n",
    "def test(net, data):\n",
    "        fit = []\n",
    "        fit = torch.FloatTensor(fit).cuda()\n",
    "        for i, (val_x,val_y) in enumerate(test_loader):\n",
    "            val_y.requires_grad=False\n",
    "            val_y = val_y[:,0] # resize the value as vector\n",
    "            pred = []\n",
    "            pred = torch.FloatTensor(pred).cuda()\n",
    "            preds = []\n",
    "            preds = torch.FloatTensor(preds).cuda()\n",
    "            for model in models:\n",
    "                pred = model(val_x)\n",
    "                pred = pred[:,0,0,0]\n",
    "                print(pred)\n",
    "                preds += pred\n",
    "            fit += loss_func(preds, val_y)\n",
    "            slope, intercept, r_value, p_value, std_err += stats.linregress(preds,val_y)\n",
    "            print(fit,r_value)\n",
    "        return fit,preds,r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit,preds,r_value = test(models, data)\n",
    "print(fit,preds,r_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions vs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x = Y_pred.flatten()\n",
    "y = Y[test_inds].flatten()\n",
    "\n",
    "# calculate R^2\n",
    "r2 = scipy.stats.pearsonr(x, y)[0]**2\n",
    "\n",
    "\n",
    "g = sns.jointplot(x,\n",
    "                  y,\n",
    "                  stat_func = None,\n",
    "                  kind = 'scatter',\n",
    "                  s = 5,\n",
    "                  alpha = 0.1,\n",
    "                  size = 5)\n",
    "\n",
    "g.ax_joint.set_xlabel('Predicted log$_2$ Growth Rate')\n",
    "g.ax_joint.set_ylabel('Measured log$_2$ Growth Rate')\n",
    "\n",
    "\n",
    "text = \"R$^2$ = {:0.2}\".format(r2)\n",
    "plt.annotate(text, xy=(-5.5, 0.95), xycoords='axes fraction')\n",
    "\n",
    "plt.title(\"CNN predictions vs. test set\", x = -3, y = 1.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
