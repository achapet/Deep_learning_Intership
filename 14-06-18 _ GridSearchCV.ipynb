{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import theano\n",
    "\n",
    "#Importing Torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from skorch.net import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA initializing\n",
    "We want to build a device-agnostic code. </br>\n",
    "- using the documentation: https://pytorch.org/docs/master/notes/cuda.html\n",
    "- requires to run argparse : see tutorial https://docs.python.org/2/howto/argparse.html (I understand what it does now â€” don't think it's super useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid the feature matrix\n",
    "data = pd.read_csv('/home/xsong/Alma/2017---Deep-learning-yeast-UTRs/Data/Random_UTRs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of the sequences.\n",
    "\n",
    "i.e. we're converting the sequences from being represented as a 50 character string of bases to a 4x50 matrix of 1's and 0's, with each row corresponding to a base and every column a position in the UTR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate different data sets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sorted numpy array of UTR indexes, from least reads to most reads\n",
    "sorted_inds = data.sort_values('t0').index.values\n",
    "\n",
    "train_inds = sorted_inds[:int(0.95*len(sorted_inds))] # 95% of the data as the training set\n",
    "\n",
    "test_inds = sorted_inds[int(0.95*len(sorted_inds)):] # UTRs with most reads at time point 0 as the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[279798 363120 118098 ... 438117  73271 122431]\n",
      "[ 56861 207040 391302 ... 221380  65465 232749]\n"
     ]
    }
   ],
   "source": [
    "print(train_inds)\n",
    "print(test_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>One-hot encoding of the different data sets</b><br>\n",
    "Each has a specific name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the work of Cuperus et al.\n",
    "# one hot encoding of UTRs\n",
    "# X = one hot encoding matrix\n",
    "# Y = growth rates\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "\n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "\n",
    "    n = len(df)\n",
    "    \n",
    "    # length of the UTR sequence\n",
    "    # we also add 10 empty spaces to either side\n",
    "    total_width = df[seq_column].str.len().max() + 20\n",
    "    \n",
    "    # initialize an empty numpy ndarray of the appropriate size\n",
    "    X = np.zeros((n, 1, 4, total_width))\n",
    "    \n",
    "    # an array with the sequences that we will one-hot encode\n",
    "    seqs = df[seq_column].values\n",
    "    \n",
    "    # loop through the array of sequences to create an array that keras will actually read\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        \n",
    "        # loop through each individual sequence, from the 5' to 3' end\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[i, 0, base_dict[seq[b]], int(b + round((total_width - len(seq))/2.))] = 1.\n",
    "    \n",
    "        # keep track of where we are\n",
    "        if (i%100000)==0:\n",
    "            print(i),\n",
    "        \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    Y = np.array(df[expression].values,\n",
    "                   dtype = theano.config.floatX)[:, np.newaxis]\n",
    "    \n",
    "    return X, Y, total_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "X, Y, total_width = one_hot_encoding(data, 'UTR', 'growth_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "Y_torch = torch.from_numpy(Y).float()\n",
    "print(type(Y_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "X_torch = torch.from_numpy(X).float()\n",
    "print(type(X_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buid the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams= {'conv_width' : [9, 13, 17, 25],\n",
    "               'conv_filters' : [32, 64, 128, 256],\n",
    "               'conv_layers' : [2, 3, 4],\n",
    "               'dense_layers' : [1, 2],\n",
    "               'conv_dropout' : [None, 0.15],\n",
    "               'dense_dropout' : [None, 0.1, 0.25, 0.5],\n",
    "               'dense_units' : [32, 64, 128, 256]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d2dae9dfd95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m  \u001b[0;31m#      return num_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d2dae9dfd95d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# input channel, output channels = number of filters, convolution kernel size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_filters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\u001b[0m\n\u001b[1;32m    295\u001b[0m         super(Conv2d, self).__init__(\n\u001b[1;32m    296\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             False, _pair(0), groups, bias)\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_channels\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in_channels must be divisible by groups'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out_channels must be divisible by groups'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for %: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super(Net, self).__init__()\n",
    "        # input channel, output channels = number of filters, convolution kernel size\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, hyperparams['conv_filters'], [4, hyperparams['conv_width']])\n",
    "        self.conv2 = nn.Conv2d(1, 16, [1,hyperparams['conv_width']])\n",
    "        self.conv3 = nn.Conv2d(1, 16, [1,hyperparams['conv_width']])\n",
    "        self.fc1 = nn.Linear(1, 12)\n",
    "        self.lin_out1 = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.lol1(x)\n",
    "        x = nn.Dropout(p=hyperparams['conv_dropout']) #\n",
    "\n",
    "        return x\n",
    "\n",
    " #   def num_flat_features(self, x):\n",
    " #      size = x.size()[1:]  # all dimensions except the batch dimension\n",
    " #       num_features = 1\n",
    " #       for s in size:\n",
    " #           num_features *= s\n",
    " #      return num_features\n",
    "\n",
    "net = Net(data)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Choice of optimizer & loss function => MSE \n",
    "# Using backpropagation\n",
    "\n",
    "# define model\n",
    "model = net\n",
    "\n",
    "# define loss function\n",
    "loss_func = nn.MSELoss() \n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.0001)\n",
    "track_loss = []\n",
    "\n",
    "#Verification & Training\n",
    "for epoch in range(2):  # loop over the dataset multiple time\n",
    "        target = Y_torch [train_inds]\n",
    "        input = X_torch [train_inds]\n",
    "        target.requires_grad=False\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = net(input)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #input = torch.randn(3, 5, requires_grad=True)\n",
    "\n",
    "        \n",
    "        #Store MSE value\n",
    "        track_loss.append(loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Skorch\n",
    "## Either this or next possibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model\n",
    "model = net\n",
    "\n",
    "# define loss function\n",
    "loss_func = nn.MSELoss() \n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.0001)\n",
    "track_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import skorch\n",
    "from skorch.net import NeuralNetClassifier\n",
    "from skorch.history import History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net = NeuralNetClassifier(\n",
    "        module=net, \n",
    "        criterion=nn.MSELoss(), \n",
    "        optimizer='torch.optim.sgd.SGD', \n",
    "        lr=0.01, \n",
    "        max_epochs=10, \n",
    "        batch_size=128, \n",
    "        iterator_train= 'torch.utils.X_torch.dataloader.DataLoader', \n",
    "        iterator_valid= 'torch.utils.X_torch.dataloader.DataLoader', \n",
    "        dataset='skorch.dataset.Dataset', \n",
    "        train_split=skorch.dataset.CVSplit(X_torch[train_inds]), \n",
    "        callbacks=None, \n",
    "        device = 'cpu',\n",
    "        verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "net.fit(X_torch, \n",
    "        y='none', \n",
    "        inputs=X_torch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_pred = net.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridsearchCV \n",
    "## I wonder if this is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(net, hyperparams, refit=False, cv=3, scoring='accuracy')\n",
    "\n",
    "gs.fit(X[train_inds], Y[train_inds])\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = net.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions vs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x = Y_pred.flatten()\n",
    "y = Y.flatten()\n",
    "\n",
    "# calculate R^2\n",
    "r2 = scipy.stats.pearsonr(x, y)[0]**2\n",
    "\n",
    "\n",
    "g = sns.jointplot(x,\n",
    "                  y,\n",
    "                  stat_func = None,\n",
    "                  kind = 'scatter',\n",
    "                  s = 5,\n",
    "                  alpha = 0.1,\n",
    "                  size = 5)\n",
    "\n",
    "g.ax_joint.set_xlabel('Predicted log$_2$ Growth Rate')\n",
    "g.ax_joint.set_ylabel('Measured log$_2$ Growth Rate')\n",
    "\n",
    "\n",
    "text = \"R$^2$ = {:0.2}\".format(r2)\n",
    "plt.annotate(text, xy=(-5.5, 0.95), xycoords='axes fraction')\n",
    "\n",
    "plt.title(\"CNN predictions vs. test set\", x = -3, y = 1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of the loss function\n",
    "plt.plot(Loss function)\n",
    "plt.ylabel('Value of the loss function')\n",
    "plt.xlabel('Time (epochs)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
