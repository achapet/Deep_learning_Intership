{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import theano\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "from skorch.net import NeuralNetClassifier\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA initializing\n",
    "We want to build a device-agnostic code. </br>\n",
    "- using the documentation: https://pytorch.org/docs/master/notes/cuda.html\n",
    "- requires to run argparse : see tutorial https://docs.python.org/2/howto/argparse.html (I understand what it does now â€” don't think it's super useful)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # If CUDA is available => cuda:0 prints\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid the feature matrix\n",
    "data = pd.read_csv('/home/xsong/Alma/2017---Deep-learning-yeast-UTRs/Data/Random_UTRs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of the sequences.\n",
    "\n",
    "i.e. we're converting the sequences from being represented as a 50 character string of bases to a 4x50 matrix of 1's and 0's, with each row corresponding to a base and every column a position in the UTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the work of Cuperus et al.\n",
    "# one hot encoding of UTRs\n",
    "# X = one hot encoding matrix\n",
    "# Y = growth rates\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "\n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "\n",
    "    n = len(df)\n",
    "    \n",
    "    # length of the UTR sequence\n",
    "    # we also add 10 empty spaces to either side\n",
    "    total_width = df[seq_column].str.len().max() + 20\n",
    "    \n",
    "    # initialize an empty numpy ndarray of the appropriate size\n",
    "    X = np.zeros((n, 1, 4, total_width))\n",
    "    \n",
    "    # an array with the sequences that we will one-hot encode\n",
    "    seqs = df[seq_column].values\n",
    "    \n",
    "    # loop through the array of sequences to create an array that keras will actually read\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        \n",
    "        # loop through each individual sequence, from the 5' to 3' end\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[i, 0, base_dict[seq[b]], int(b + round((total_width - len(seq))/2.))] = 1.\n",
    "    \n",
    "        # keep track of where we are\n",
    "        if (i%100000)==0:\n",
    "            print(i),\n",
    "        \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    Y = np.array(df[expression].values,\n",
    "                   dtype = theano.config.floatX)[:, np.newaxis]\n",
    "    \n",
    "    return X, Y, total_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "X, Y, total_width = one_hot_encoding(data, 'UTR', 'growth_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.from_numpy(X).float().cuda() #change to torch and upload to CUDA\n",
    "Y_torch = torch.from_numpy(Y).float().cuda() #change to torch and upload to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sorted numpy array of UTR indexes, from least reads to most reads\n",
    "sorted_inds = data.sort_values('t0').index.values\n",
    "train_inds = sorted_inds[:int(0.95*len(sorted_inds))] # 95% of the data as the training set\n",
    "test_inds = sorted_inds[int(0.95*len(sorted_inds)):] # UTRs with most reads at time point 0 as the test set\n",
    "\n",
    "# set the seed before randomly shuffling the data\n",
    "seed = 0.5\n",
    "random.shuffle(train_inds, lambda :seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model\n",
    "\n",
    "I need to figure out how to make the dropout happen and Flatten. \n",
    "How do hidden units work in fully connected layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buid the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 1, kernel_size=[4, 13], stride=(1, 1))\n",
      "  (conv2): Conv2d(1, 1, kernel_size=[1, 13], stride=(1, 1))\n",
      "  (conv3): Conv2d(1, 1, kernel_size=[1, 13], stride=(1, 1))\n",
      "  (fc1): Linear(in_features=34, out_features=120, bias=True)\n",
      "  (lin_out1): Linear(in_features=120, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=1\n",
    "batch_size=10\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super(Net, self).__init__()\n",
    "        # input channel, output channels = number of filters, convolution kernel size\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, size, [4,13])\n",
    "        self.conv2 = nn.Conv2d(1, size, [1,13])\n",
    "        self.conv3 = nn.Conv2d(1, size, [1,13])\n",
    "        self.fc1 = nn.Linear(34, 120)\n",
    "        self.lin_out1 = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print('conv1',x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print('conv2',x.size())\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print('conv3',x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print('fc1',x.size())\n",
    "        x = self.lin_out1(x)\n",
    "        #print('lol1',x.size())\n",
    "        #x = nn.Dropout(p=0.15)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net(data)\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "next(net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([1, 1, 4, 13])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def eval(model, data):\n",
    "        model.eval()\n",
    "        ######EVALUATION STEP#################\n",
    "        #same as during training step, need to do batch training\n",
    "        correct_count = 0.\n",
    "        total_count = 0.\n",
    "        for i, (dev_data_x,dev_data_y) in enumerate(train_loader):\n",
    "            train_data_y.requires_grad=False\n",
    "            train_data_y = train_data_y[:,0].float()                 # resize the value as vector\n",
    "            optimizer.zero_grad()      # zero the parameter gradients\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            pred = net(dev_data_x)\n",
    "            loss = nn.MSELoss(pred, train_data_y)\n",
    "            output = output[:,0,0,0]   #resize the output as vector\n",
    "            correct_count += torch.sum(torch.max(pred, 1)[1] == dev_data_y).data[0]\n",
    "            total_count += batch_data.shape[0]\n",
    "        return correct_count, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-cbe6069d8ac4>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-cbe6069d8ac4>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    dev_data_x = x[dev_index]      #evalutation data\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Choice of optimizer & loss function => MSE \n",
    "# Using backpropagation\n",
    "\n",
    "# Initiate the hyperparameters\n",
    "number_epochs = 20\n",
    "track_loss = []\n",
    "loss_func = nn.MSELoss().cuda()\n",
    "k_fold = 10\n",
    "\n",
    "# Define dataset and initialize mini-batch data\n",
    "x = X_torch[train_inds]\n",
    "y = Y_torch[train_inds]\n",
    "num_data = x.shape[0]\n",
    "num_dev_data = int(num_data/k_fold) #evalutation data amount\n",
    "\n",
    "#Training with crossvalidation\n",
    "for split in range(k_fold):\n",
    "    \n",
    "    dev_index = list(range(num_dev_data*split, num_dev_data*(split+1)))\n",
    "    train_index = list(range(0, num_dev_data*split))\n",
    "    train_index.append(list(range(num_dev_data*(split+1),num_dev_data*k_fold))\n",
    "                       \n",
    "    dev_data_x = x[dev_index]      #evalutation data\n",
    "    dev_data_y = y[dev_index]      #evalutation data\n",
    "\n",
    "    train_data_x = x[train_index]\n",
    "    train_data_y = y[train_index]\n",
    "    \n",
    "    train_dataset = Data.TensorDataset(train_data_x, train_data_y)\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    dev_dataset = Data.TensorDataset(dev_data_x, dev_data_y)\n",
    "    dev_loader = Data.DataLoader(dev_dataset, batch_size=128, shuffle=True)  \n",
    "    \n",
    "    learning_rate = 0.005\n",
    "    best_acc = 0.\n",
    "    optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "    for epoch in range(number_epochs): # loop over the dataset multiple time\n",
    "        print(epoch)\n",
    "        \n",
    "        #######TRAINING STEP##################\n",
    "        for i, (train_data_x,train_data_y) in enumerate(train_loader):\n",
    "            train_data_y.requires_grad=False\n",
    "            train_data_y = train_data_y[:,0].float()                 # resize the value as vector\n",
    "            optimizer.zero_grad()      # zero the parameter gradients\n",
    "\n",
    "            output = net(x)\n",
    "            output = output[:,0,0,0]   #resize the output as vector\n",
    "            loss = loss_func(output, y)# compute the loss of the system\n",
    "            loss.backward()            # start backward function\n",
    "            optimizer.step()           # optimizing step\n",
    "            if (i%1000)==0:\n",
    "                print(i,loss),\n",
    "            #Store MSE value\n",
    "            track_loss.append(loss)\n",
    "        correct_count, total_count = eval(model, dev_data)\n",
    "        acc = correct_count / total_count\n",
    "        print(\"dev acc: {}\".format(acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            print(\"save the model\")\n",
    "            torch.save(net.state_dict(), \"/home/xsong/Alma/Model_training_21-06-18_{}.pt\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'track_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-85aaa688e054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plotting of the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Value of the loss function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'track_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting of the loss function\n",
    "plt.plot(track_loss)\n",
    "plt.ylabel('Value of the loss function')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions vs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch[test_inds]\n",
    "Y_pred = net(X_torch[test_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x = Y_pred.flatten()\n",
    "y = Y[test_inds].flatten()\n",
    "\n",
    "# calculate R^2\n",
    "r2 = scipy.stats.pearsonr(x, y)[0]**2\n",
    "\n",
    "\n",
    "g = sns.jointplot(x,\n",
    "                  y,\n",
    "                  stat_func = None,\n",
    "                  kind = 'scatter',\n",
    "                  s = 5,\n",
    "                  alpha = 0.1,\n",
    "                  size = 5)\n",
    "\n",
    "g.ax_joint.set_xlabel('Predicted log$_2$ Growth Rate')\n",
    "g.ax_joint.set_ylabel('Measured log$_2$ Growth Rate')\n",
    "\n",
    "\n",
    "text = \"R$^2$ = {:0.2}\".format(r2)\n",
    "plt.annotate(text, xy=(-5.5, 0.95), xycoords='axes fraction')\n",
    "\n",
    "plt.title(\"CNN predictions vs. test set\", x = -3, y = 1.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
